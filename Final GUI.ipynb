{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "394fae94-8f98-45cc-ac48-b8fb7cff6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Directory to store datasets\n",
    "# Replace this with the path of the folder in your Jupyter Notebook\n",
    "dataset_location = '\\\\\\\\userfs\\\\lwm527\\\\w2k\\\\Advanced Programming\\\\Datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6428e5f6-8f40-4d82-9c31-9b34503301c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dropdown(new_file=None):\n",
    "    global dropdown_files, view_files, files, files2, file_var, file_var2\n",
    "    if new_file:\n",
    "        # Add the new file to the list of available files\n",
    "        files.append(new_file)\n",
    "        # Set the new file as the selected value in the first dropdown\n",
    "        file_var.set(new_file)\n",
    "        # Destroy the existing dropdown widget to recreate it with updated values\n",
    "        dropdown_files.destroy()  \n",
    "        # Create a new OptionMenu with the updated file list\n",
    "        dropdown_files = tk.OptionMenu(dropdown_frame, file_var, *files,\n",
    "                                      command=lambda event_a: selected_file(event=event_a, target_widget=content))\n",
    "        # Configure the appearance of the dropdown\n",
    "        dropdown_files.configure(bg='lightblue', relief='raised')\n",
    "        # Place the dropdown in the grid layout\n",
    "        dropdown_files.grid(row=9, column=0, sticky='N')\n",
    "\n",
    "    # Update the second dropdown for the \"Manipulation\" tab\n",
    "    files2 = find_files2()  # Get the list of files for manipulation\n",
    "    if len(files2) != 0:\n",
    "        # Set the first file in the list as the selected value\n",
    "        file_var2.set(files2[0]) \n",
    "        # Destroy the existing dropdown widget\n",
    "        view_files.destroy()\n",
    "        # Create a new OptionMenu with the updated file list\n",
    "        view_files = tk.OptionMenu(options_frame, file_var2, *files2,\n",
    "                                  command=lambda event_b: selected_file(event=event_b, target_widget=manipulation_content))\n",
    "        # Configure the appearance of the dropdown\n",
    "        view_files.configure(bg='lightblue', relief='raised')\n",
    "        # Place the dropdown in the grid layout\n",
    "        view_files.grid(row=9, column=0, columnspan=2, sticky='N')\n",
    "\n",
    "def remove_unwanted_chars(name):\n",
    "    name = name.strip() # Remove leading and following spaces\n",
    "    name = name.upper() # Make the string UPPER case\n",
    "    name = re.sub(r'\\s+', '_', name)  # Replace spaces with underscores\n",
    "    name = re.sub(r'[^A-Za-z0-9_-]', '_', name)  # Remove unwanted characters\n",
    "    return name\n",
    "\n",
    "def clean_date_time(string):\n",
    "    clean_string = re.sub(r'[^0-9/: ]', '', string)  # Remove unwanted characters\n",
    "    clean_string = re.sub(r'\\s+', ' ', string).strip()  # Remove extra spaces\n",
    "    return clean_string\n",
    "\n",
    "def format_xml(element, level=0):\n",
    "    indent = \"  \"  # Two spaces for indentation\n",
    "    if len(element):\n",
    "        # Add indentation to the element's text and tail\n",
    "        element.text = f\"\\n{indent * (level + 1)}\" if not element.text else element.text.strip()\n",
    "        # Recursively format child elements\n",
    "        for child in element:\n",
    "            format_xml(child, level + 1)\n",
    "        element.tail = f\"\\n{indent * level}\" \n",
    "    else:\n",
    "        # If the element has no children, just strip the text\n",
    "        element.text = element.text.strip() if element.text else None\n",
    "        element.tail = f\"\\n{indent * level}\" \n",
    "    return element\n",
    "\n",
    "def handle_missing_data(dataframe):\n",
    "    # Get a list of columns that have missing values (NaN).\n",
    "    missing_columns = dataframe.columns[dataframe.isnull().any()].tolist()\n",
    "    # Initialize a counter for the number of replaced values.\n",
    "    removed_or_replaced_count = 0  \n",
    "    # Iterate over each column with missing values.\n",
    "    for column in missing_columns:\n",
    "        # Count how many missing values are in the current column.\n",
    "        replaced_count = dataframe[column].isnull().sum()\n",
    "        # Fill all missing values in the column with the string \"Unknown\".\n",
    "        dataframe[column] = dataframe[column].fillna(\"Unknown\")\n",
    "        # Update the total count of replaced values.\n",
    "        removed_or_replaced_count += replaced_count\n",
    "    return dataframe, removed_or_replaced_count\n",
    "\n",
    "def standardize_data(dataframe):\n",
    "    today = pd.Timestamp.today() # Get today's date.\n",
    "    too_old_date = pd.Timestamp('2022-01-01') # Define a cutoff date for old data.\n",
    "    removed_or_replaced_count = 0 # Initialize a counter.\n",
    "\n",
    "    # Iterate over each column in the DataFrame and standardize accordingly\n",
    "    for column in dataframe.columns:\n",
    "        if \"DATE\" in column:\n",
    "            dataframe[column] = dataframe[column].apply(lambda x: clean_date_time(str(x)))\n",
    "            dataframe[column] = pd.to_datetime(dataframe[column], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "            dataframe[column] = dataframe[column].apply(lambda x: x if (x >= too_old_date) & (x <= today) else pd.NaT)\n",
    "            replaced_count = dataframe[column].isna().sum()\n",
    "            dataframe[column] = dataframe[column].fillna(\"Unknown\")\n",
    "            dataframe[column] = dataframe[column].apply(lambda x: x.strftime('%d/%m/%Y') if x != \"Unknown\" else x)\n",
    "            removed_or_replaced_count += replaced_count\n",
    "        elif \"TIME\" in column:\n",
    "            dataframe[column] = dataframe[column].apply(lambda x: clean_date_time(str(x)))\n",
    "            dataframe[column] = pd.to_datetime(dataframe[column], errors='coerce', format='%H:%M:%S')\n",
    "            replaced_count = dataframe[column].isna().sum()\n",
    "            dataframe[column] = dataframe[column].fillna(\"Unknown\")\n",
    "            dataframe[column] = dataframe[column].apply(lambda x: x.strftime('%H:%M:%S') if x != \"Unknown\" else x)\n",
    "            removed_or_replaced_count += replaced_count\n",
    "        elif \"USER_FULL_NAME__ANONYMIZED\" in column:\n",
    "            replaced_count = dataframe[column].apply(lambda x: not(str(x).isnumeric() or str(x)=='Unknown')).sum()\n",
    "            dataframe[column] = dataframe[column].apply(lambda x: x if str(x).isnumeric() or str(x)=='Unknown' else 'Unknown')\n",
    "            removed_or_replaced_count += replaced_count\n",
    "        else:\n",
    "            dataframe[column] = dataframe[column].astype(str).apply(remove_unwanted_chars)\n",
    "            \n",
    "    return dataframe, removed_or_replaced_count\n",
    "\n",
    "def clean():\n",
    "    # Retrieve the currently selected file name from the dropdown\n",
    "    file_name = file_var.get()\n",
    "    file_path = os.path.join(dataset_location, file_name)  # Construct full file path\n",
    "    extension = os.path.splitext(file_name)[1].lower()  # Get file extension\n",
    "    clean_file_name = f\"{os.path.splitext(file_name)[0]}_clean{extension}\"  # Create cleaned file name\n",
    "    clean_file_path = os.path.join(dataset_location, clean_file_name)  # Construct cleaned file path\n",
    "    removed_or_replaced_count = 0  # Counter for data cleaning operations\n",
    "    continue_cleaning = True  # Flag to determine if cleaning should proceed\n",
    "    \n",
    "    # Check if the file has already been cleaned\n",
    "    if \"clean\" in file_name.lower():\n",
    "        messagebox.showerror(\"Error\", \"This file has already been cleaned.\")\n",
    "        continue_cleaning = False\n",
    "\n",
    "    # Check if the cleaned file already exists\n",
    "    if os.path.exists(clean_file_path):\n",
    "        messagebox.showerror(\"Error\", f\"The cleaned file '{clean_file_name}' already exists.\")\n",
    "        continue_cleaning = False\n",
    "\n",
    "    # Proceed with cleaning if no errors\n",
    "    if continue_cleaning:\n",
    "        try:\n",
    "            # Load the file based on its extension\n",
    "            if extension == \".csv\":\n",
    "                df = pd.read_csv(file_path)  # Read CSV into a DataFrame\n",
    "            elif extension == \".json\":\n",
    "                # Read JSON file and load into DataFrame\n",
    "                with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    df = pd.DataFrame(data[\"Records\"] if \"Records\" in data else data)\n",
    "            elif extension == \".xml\":\n",
    "                # Parse XML file and load into DataFrame\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                records = []\n",
    "                for record in root.findall(\"Record\"):\n",
    "                    # Create dictionary for each XML record\n",
    "                    row = {child.tag: child.text.strip() if child.text else None for child in record}\n",
    "                    records.append(row)\n",
    "                df = pd.DataFrame(records)  # Convert list of records into a DataFrame\n",
    "            else:\n",
    "                # Unsupported file format error\n",
    "                raise ValueError(\"Unsupported file format for cleaning\")\n",
    "\n",
    "            # Remove unwanted characters from column names\n",
    "            df.columns = [remove_unwanted_chars(col) for col in df.columns]\n",
    "            initial_row_count = len(df)  # Count the number of rows before cleaning\n",
    "        \n",
    "            # Replace missing data and count replaced values\n",
    "            df, missing_data_count = handle_missing_data(df)\n",
    "            removed_or_replaced_count += missing_data_count\n",
    "\n",
    "            # Validate and standardize data, count standardization changes\n",
    "            df, standardization_count = standardize_data(df)\n",
    "            removed_or_replaced_count += standardization_count\n",
    "            \n",
    "            # Replace or remove remaining invalid data\n",
    "            rows_na = df.isna().sum().sum()  # Count remaining missing data\n",
    "            df = df.fillna('Unknown')  # Replace missing values with 'Unknown'\n",
    "            removed_or_replaced_count += rows_na\n",
    "    \n",
    "            # Save cleaned file based on its original format\n",
    "            if extension == \".csv\":\n",
    "                df.to_csv(clean_file_path, index=False)  # Save as CSV\n",
    "            elif extension == \".json\":\n",
    "                # Convert DataFrame to JSON format and save\n",
    "                cleaned_data = df.to_dict(orient=\"records\")\n",
    "                with open(clean_file_path, 'w', encoding='utf-8') as json_file:\n",
    "                    json.dump({\"Records\": cleaned_data}, json_file, indent=4)\n",
    "            elif extension == \".xml\":\n",
    "                # Convert DataFrame to XML format and save\n",
    "                root = ET.Element(\"Records\")\n",
    "                for index, row in df.iterrows():\n",
    "                    record = ET.SubElement(root, \"Record\")\n",
    "                    for col, value in row.items():\n",
    "                        child = ET.SubElement(record, col)\n",
    "                        child.text = str(value) if pd.notnull(value) else \"\"\n",
    "                format_xml(root)  # Format XML output\n",
    "                with open(clean_file_path, 'w', encoding='utf-8') as xml_file:\n",
    "                    xml_file.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "                    xml_file.write(ET.tostring(root, encoding=\"unicode\", method=\"xml\"))\n",
    "\n",
    "            # Update the dropdown menu and display success message\n",
    "            update_dropdown(clean_file_name)  # Add cleaned file to dropdown\n",
    "            selected_file(clean_file_name, target_widget=content)  # Automatically select cleaned file\n",
    "            messagebox.showinfo(\"Success\", f\"File '{file_name}' has been cleaned successfully!\\n\"\n",
    "                                          f\"{removed_or_replaced_count} records have been replaced.\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors during the cleaning process\n",
    "            messagebox.showerror(\"Error\", f\"Failed to clean file: {e}\")\n",
    "\n",
    "def save_as_xml():\n",
    "    # Get the selected file name from the dropdown\n",
    "    file_name = file_var.get()\n",
    "\n",
    "    # Check if the file is a CSV (only CSV files are allowed for conversion to XML)\n",
    "    if os.path.splitext(file_name)[1].lower() == '.csv':\n",
    "        csv_path = os.path.join(dataset_location, file_name)  # Construct full path of the CSV file\n",
    "        xml_file_name = os.path.splitext(file_name)[0] + \".xml\"  # Define the XML file name\n",
    "        xml_path = os.path.join(dataset_location, xml_file_name)  # Construct full path of the XML file\n",
    "        \n",
    "        try:\n",
    "            # Open the CSV file and read its contents\n",
    "            with open(csv_path, 'r') as csv_file:\n",
    "                csv_reader = csv.reader(csv_file)\n",
    "                column_names = next(csv_reader)  # Read the column headers\n",
    "                root = ET.Element(\"Root\")  # Create the root element for the XML file\n",
    "                \n",
    "                # Iterate over each row in the CSV file\n",
    "                for row in csv_reader:\n",
    "                    record = ET.SubElement(root, \"Record\")  # Create a \"Record\" element for each row\n",
    "                    for col_name, value in zip(column_names, row):\n",
    "                        # Create child elements for each column in the row\n",
    "                        child = ET.SubElement(record, remove_unwanted_chars(col_name))\n",
    "                        child.text = value  # Set the text of the child element to the cell value\n",
    "\n",
    "                # Format the XML for better readability\n",
    "                format_xml(root)\n",
    "                \n",
    "                # Write the XML content to the file\n",
    "                with open(xml_path, 'w', encoding='utf-8') as xml_file:\n",
    "                    xml_file.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')  # Add XML declaration\n",
    "                    xml_file.write(ET.tostring(root, encoding=\"unicode\", method=\"xml\"))\n",
    "        \n",
    "            # Update the dropdown menu and automatically select the new XML file\n",
    "            update_dropdown(xml_file_name)\n",
    "            selected_file(xml_file_name, target_widget=content)\n",
    "            \n",
    "            # Show a success message\n",
    "            messagebox.showinfo(\"Success\", f\"File reshaped to XML format and saved as {xml_path}\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors during the conversion process\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save as XML: {e}\")\n",
    "    else:\n",
    "        # Display an error if the selected file is not a CSV\n",
    "        messagebox.showerror(\"Error\", \"Only CSV files are allowed for transformation\")\n",
    "\n",
    "def save_as_json():\n",
    "    # Get the selected file name from the dropdown\n",
    "    file_name = file_var.get()\n",
    "\n",
    "    # Check if the file is a CSV (only CSV files are allowed for conversion to JSON)\n",
    "    if os.path.splitext(file_name)[1].lower() == '.csv':\n",
    "        csv_path = os.path.join(dataset_location, file_name)  # Construct full path of the CSV file\n",
    "        json_file_name = os.path.splitext(file_name)[0] + \".json\"  # Define the JSON file name\n",
    "        json_path = os.path.join(dataset_location, json_file_name)  # Construct full path of the JSON file\n",
    "\n",
    "        try:\n",
    "            # Open the CSV file and read its contents\n",
    "            with open(csv_path, 'r') as csv_file:\n",
    "                csv_reader = csv.reader(csv_file)\n",
    "                column_names = next(csv_reader)  # Read the column headers\n",
    "                records = []\n",
    "\n",
    "                # Iterate over each row in the CSV file\n",
    "                for row in csv_reader:\n",
    "                    # Create a dictionary for each row, with column names as keys\n",
    "                    record = {remove_unwanted_chars(key): value for key, value in zip(column_names, row)}\n",
    "                    records.append(record)  # Add the dictionary to the list of records\n",
    "\n",
    "            # Prepare the data in JSON format\n",
    "            data = {\"Records\": records}\n",
    "\n",
    "            # Write the JSON content to the file\n",
    "            with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(data, json_file, indent=4)  # Save JSON with indentation for readability\n",
    "    \n",
    "            # Update the dropdown menu and automatically select the new JSON file\n",
    "            update_dropdown(json_file_name)\n",
    "            selected_file(json_file_name, target_widget=content)\n",
    "            \n",
    "            # Show a success message\n",
    "            messagebox.showinfo(\"Success\", f\"File reshaped to JSON format and saved as {json_path}\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors during the conversion process\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save as JSON: {e}\")\n",
    "    else:\n",
    "        # Display an error if the selected file is not a CSV\n",
    "        messagebox.showerror(\"Error\", \"Only CSV files are allowed for transformation\")\n",
    "\n",
    "def selected_file(event=None, target_widget=None):\n",
    "    # Determine which widget to update (default to `manipulation_content` if not specified)\n",
    "    target_widget = target_widget if target_widget else manipulation_content\n",
    "\n",
    "    if target_widget == content:\n",
    "        # If the content widget is selected, set the selected file\n",
    "        file_selected = event if event else file_var.get()\n",
    "        selection_label.configure(text=f'SELECTED FILE: {file_selected}', background='lightgreen')\n",
    "        \n",
    "        # Show the clean, save-as-XML, and save-as-JSON buttons\n",
    "        clean_button.grid(row=22, column=0, padx=5, pady=5, sticky='EW')\n",
    "        xml_button.grid(row=22, column=1, padx=5, pady=5, sticky='EW')\n",
    "        json_button.grid(row=22, column=2, padx=5, pady=5, sticky='EW')\n",
    "    else:\n",
    "        # Otherwise, set the selected file for `manipulation_content`\n",
    "        file_selected = event if event else file_var2.get()\n",
    "        selection_label2.configure(text=f'SELECTED FILE: {file_selected}', background='lightgreen')\n",
    "    \n",
    "    file_path = os.path.join(dataset_location, file_selected)  # Construct full file path\n",
    "    extension = os.path.splitext(file_selected)[1].lower()  # Get the file extension\n",
    "\n",
    "    try:\n",
    "        # Load and display the file content in the target widget\n",
    "        target_widget.configure(state=tk.NORMAL)\n",
    "        target_widget.delete(1.0, tk.END)\n",
    "\n",
    "        if extension == '.csv':\n",
    "            # Read and display CSV content\n",
    "            with open(file_path, 'r') as file:\n",
    "                target_widget.insert(tk.END, file.read())\n",
    "        elif extension == '.xml':\n",
    "            # Read and display XML content\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            format_xml(root)  # Format XML for better readability\n",
    "            xml_declaration = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n",
    "            xml_string = f\"{xml_declaration}{ET.tostring(root, encoding='unicode', method='xml')}\"\n",
    "            target_widget.insert(tk.END, xml_string)\n",
    "        elif extension == '.json':\n",
    "            # Read and display JSON content\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                json_string = json.dumps(json_data, indent=4)\n",
    "                target_widget.insert(tk.END, json_string)\n",
    "        else:\n",
    "            # Unsupported file format\n",
    "            target_widget.insert(tk.END, \"Unsupported file format\")\n",
    "        \n",
    "        # Make the target widget read-only\n",
    "        target_widget.configure(state=tk.DISABLED)\n",
    "    except Exception as e:\n",
    "        # Handle any errors during the file reading process\n",
    "        messagebox.showerror(\"Error\", f\"Failed to read file: {e}\")\n",
    "\n",
    "# Function to upload a CSV file to the dataset location\n",
    "def upload_file():\n",
    "    # Open a file dialog to allow the user to select a file\n",
    "    file_path = filedialog.askopenfilename(title='Choose a File', filetypes=[(\"CSV Files\", \"*.csv\")])\n",
    "\n",
    "    try:\n",
    "        # Copy the selected file to the dataset location\n",
    "        name = os.path.basename(file_path)  # Extract the file name from the path\n",
    "        destination = os.path.join(dataset_location, name)  # Construct destination path\n",
    "        with open(file_path, 'r') as chosen_file:\n",
    "            with open(destination, 'x') as destination_file:\n",
    "                destination_file.write(chosen_file.read())  # Copy the file contents\n",
    "        \n",
    "        # Update the dropdown menu with the new file\n",
    "        update_dropdown(name)\n",
    "        selected_file(name, target_widget=content)  # Automatically select the uploaded file\n",
    "        \n",
    "        # Show a success message\n",
    "        messagebox.showinfo(\"Upload Successful\", f\"File '{name}' uploaded successfully!\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors during the upload process\n",
    "        messagebox.showerror(\"Upload Failed\", f\"Failed to upload file: {e}\")\n",
    "\n",
    "#GUI\n",
    "gui = tk.Tk()\n",
    "gui.title(\"File Manipulation\")\n",
    "gui.geometry(\"1100x900\")\n",
    "\n",
    "#Creating Tabs within the GUI\n",
    "notebook = ttk.Notebook(gui)\n",
    "notebook.pack(expand=True, fill=\"both\")\n",
    "upload_tab = ttk.Frame(notebook)\n",
    "notebook.add(upload_tab, text=\"Upload\")\n",
    "manipulation_tab = ttk.Frame(notebook)\n",
    "notebook.add(manipulation_tab, text=\"Manipulation\")\n",
    "statistics_tab = ttk.Frame(notebook)\n",
    "notebook.add(statistics_tab, text=\"Statistics\")\n",
    "\n",
    "# Styles\n",
    "style = ttk.Style()\n",
    "style.configure('TFrame', background='LightGray')\n",
    "style.configure('TLabel', background='White', relief='raised', font=('Arial', 10))\n",
    "style.configure('Custom.TLabel', font=('Arial', 10, 'bold'), background='LightGray', relief='flat')\n",
    "\n",
    "# Dropdown frame\n",
    "dropdown_frame = ttk.Frame(upload_tab, padding=10, relief=\"ridge\")\n",
    "dropdown_frame.grid(row=0, column=0, padx=10, pady=10, sticky='NEWS')\n",
    "\n",
    "# Text frame\n",
    "text_frame = ttk.Frame(upload_tab, padding=10, relief=\"ridge\")\n",
    "text_frame.grid(row=0, column=1, padx=10, pady=10, sticky='NEWS')\n",
    "\n",
    "# Text and Dropdown Grid configuration\n",
    "for i in range(0, 22):\n",
    "    dropdown_frame.rowconfigure(i, weight=1)\n",
    "    text_frame.rowconfigure(i, weight=1)\n",
    "dropdown_frame.columnconfigure(0, weight=1)\n",
    "for i in range(0, 3):\n",
    "    text_frame.columnconfigure(i, weight=1)\n",
    "\n",
    "# Dropdown label\n",
    "dropdown_label = ttk.Label(dropdown_frame, text=\"Choose a File\", anchor='center', style='Custom.TLabel')\n",
    "dropdown_label.grid(row=8, column=0, padx=5, pady=5, sticky='EW')\n",
    "\n",
    "# Selected file label\n",
    "selection_label = ttk.Label(dropdown_frame, text='SELECTED FILE: ', anchor='center', background='white')\n",
    "selection_label.grid(row=11, column=0, padx=5, pady=5, sticky='EW')\n",
    "\n",
    "# Text label for file contents\n",
    "text_label = ttk.Label(text_frame, text=\"File Contents\", anchor='center', relief=\"solid\", borderwidth=1)\n",
    "text_label.grid(row=0, column=0, columnspan=3, padx=5, pady=5, sticky='NEWS')\n",
    "\n",
    "# Dropdown menu for file selection\n",
    "file_var = tk.StringVar()\n",
    "file_var.set(\"\")\n",
    "files = [file for file in os.listdir(dataset_location) if os.path.isfile(os.path.join(dataset_location, file))]\n",
    "if not files:\n",
    "    dropdown_files = tk.OptionMenu(dropdown_frame, file_var, \"\")\n",
    "else:\n",
    "    dropdown_files = tk.OptionMenu(dropdown_frame, file_var, *files, \n",
    "                                   command=lambda event_a: selected_file(event=event_a, target_widget=content))\n",
    "dropdown_files.configure(bg='lightblue', relief='raised')\n",
    "dropdown_files.grid(row=9, column=0, sticky='EW')\n",
    "\n",
    "# Text widget to display file contents\n",
    "content = tk.Text(text_frame, wrap=tk.WORD, state=tk.DISABLED, relief=\"solid\", borderwidth=1)\n",
    "content.grid(row=1, column=0, columnspan=3, rowspan=20, padx=5, pady=5, sticky=tk.NSEW)\n",
    "\n",
    "# Upload button\n",
    "upload_button = tk.Button(dropdown_frame, text=\"Upload Your Own\", command=upload_file, bg=\"lightblue\", relief=\"raised\")\n",
    "upload_button.grid(row=10, column=0, padx=5, pady=10, sticky='EW')\n",
    "\n",
    "# Clean and save buttons\n",
    "clean_button = tk.Button(text_frame, text=\"Clean Data\", command=clean, bg=\"lightblue\", relief=\"raised\")\n",
    "xml_button = tk.Button(text_frame, text=\"Save as XML\", command=save_as_xml, bg=\"lightblue\", relief=\"raised\")\n",
    "json_button = tk.Button(text_frame, text=\"Save as JSON\", command=save_as_json, bg=\"lightblue\", relief=\"raised\")\n",
    "\n",
    "# Initially hide the buttons\n",
    "clean_button.grid_remove()\n",
    "xml_button.grid_remove()\n",
    "json_button.grid_remove()\n",
    "\n",
    "# Configure grid weights for the main window\n",
    "upload_tab.rowconfigure(0, weight=1)\n",
    "upload_tab.columnconfigure(0, weight=1)\n",
    "upload_tab.columnconfigure(1, weight=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "113fc3f0-c6f8-4d01-aeaf-5e0884f203eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_selected():\n",
    "    update_dropdown()  # Refresh the dropdown menu with updated options.\n",
    "    selected_file(target_widget=manipulation_content)  # Display the content of the selected file.\n",
    "\n",
    "def find_files():\n",
    "    selected_format = format_var.get()  # Get the selected file format.\n",
    "    files_to_manipulate = []\n",
    "    for file in os.listdir(dataset_location):  # Iterate over files in the dataset directory.\n",
    "        if selected_format.lower() in file.lower() and 'clean' in file.lower():  # Match format and 'clean' keyword.\n",
    "            files_to_manipulate.append(os.path.join(dataset_location, file))  # Add matching files to the list.\n",
    "    return files_to_manipulate\n",
    "\n",
    "def find_files2():\n",
    "    selected_format = format_var.get()  # Get the selected file format.\n",
    "    files_to_show = []\n",
    "    for file in os.listdir(dataset_location):  # Iterate over files in the dataset directory.\n",
    "        if selected_format.lower() in file.lower() and 'clean' in file.lower():  # Match 'clean' files.\n",
    "            files_to_show.append(file)\n",
    "        elif selected_format.lower() in file.lower() and 'merged_dataset' in file.lower():  # Match merged files.\n",
    "            files_to_show.append(file)\n",
    "    return files_to_show\n",
    "\n",
    "def remove_records():\n",
    "    selected_format = format_var.get()  # Get the selected file format.\n",
    "    files_to_manipulate = find_files()  # Get files matching the criteria.\n",
    "    if len(files_to_manipulate) != 3:  # Ensure there are exactly three files to manipulate.\n",
    "        messagebox.showerror(\"Error\", \"Could not find exactly three 'clean' files for manipulation.\")\n",
    "    else:\n",
    "        try:\n",
    "            user_df = None\n",
    "            user_path = None\n",
    "            user_data = None\n",
    "            indexes = None\n",
    "            for file_path in files_to_manipulate:\n",
    "                if selected_format == 'JSON':\n",
    "                    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                        data = json.load(json_file)  # Load JSON file data.\n",
    "                        df = pd.DataFrame(data[\"Records\"] if \"Records\" in data else data)  # Convert to DataFrame.\n",
    "                        if 'COMPONENT' in df.columns and 'USER_FULL_NAME__ANONYMIZED' in df.columns:\n",
    "                            # Remove rows with 'SYSTEM' and 'FOLDER' in 'COMPONENT' column.\n",
    "                            activity_df_removed = df[~df['COMPONENT'].isin(['SYSTEM', 'FOLDER'])]\n",
    "                            indexes = df[~df['COMPONENT'].isin(['SYSTEM', 'FOLDER'])].index\n",
    "                            data[\"Records\"] = activity_df_removed.to_dict(orient=\"records\")\n",
    "                        elif 'COMPONENT' not in df.columns and 'USER_FULL_NAME__ANONYMIZED' in df.columns:\n",
    "                            # Store user-specific data and path for later processing.\n",
    "                            user_df = df\n",
    "                            user_data = data\n",
    "                            user_path = file_path\n",
    "                        elif 'COMPONENT' in df.columns and 'USER_FULL_NAME__ANONYMIZED' not in df.columns:\n",
    "                            # Remove rows with 'SYSTEM' and 'FOLDER' for other files.\n",
    "                            codes_df_removed = df[~df['COMPONENT'].isin(['SYSTEM', 'FOLDER'])]\n",
    "                            data[\"Records\"] = codes_df_removed.to_dict(orient=\"records\")\n",
    "                        if file_path != user_path:\n",
    "                            with open(file_path, \"w\") as file:\n",
    "                                json.dump(data, file, indent=4)  # Save updated JSON data.\n",
    "                elif selected_format == 'XML':\n",
    "                    df = pd.read_xml(file_path)  # Read XML file into DataFrame.\n",
    "                    if 'COMPONENT' in df.columns and 'USER_FULL_NAME__ANONYMIZED' in df.columns:\n",
    "                        # Filter out rows with specific 'COMPONENT' values.\n",
    "                        df = df[~df['COMPONENT'].isin(['SYSTEM', 'FOLDER'])]\n",
    "                        indexes = df[~df['COMPONENT'].isin(['SYSTEM', 'FOLDER'])].index\n",
    "                    elif 'COMPONENT' not in df.columns and 'USER_FULL_NAME__ANONYMIZED' in df.columns:\n",
    "                        # Store user-specific data and path for later processing.\n",
    "                        user_df = df\n",
    "                        user_path = file_path\n",
    "                    elif 'COMPONENT' in df.columns and 'USER_FULL_NAME__ANONYMIZED' not in df.columns:\n",
    "                        # Filter out rows for other files.\n",
    "                        df = df[~df['COMPONENT'].isin(['SYSTEM', 'FOLDER'])]\n",
    "                    if file_path != user_path:\n",
    "                        df.to_xml(file_path, index=False, root_name='Records', row_name='Record')  # Save XML.\n",
    "\n",
    "            if user_df is not None and indexes is not None:\n",
    "                # Update user-specific file based on filtered indexes.\n",
    "                user_df = user_df.loc[indexes]\n",
    "                if selected_format == 'JSON':\n",
    "                    user_data[\"Records\"] = user_df.to_dict(orient=\"records\")\n",
    "                    with open(user_path, \"w\") as file:\n",
    "                        json.dump(user_data, file, indent=4)\n",
    "                elif selected_format == 'XML':\n",
    "                    user_df.to_xml(user_path, index=False, root_name='Records', row_name='Record')\n",
    "                    \n",
    "            messagebox.showinfo(\"Success\", \"Records removed successfully!\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to remove records: {e}\") \n",
    "\n",
    "def rename():\n",
    "    selected_format = format_var.get()  # Get the selected file format.\n",
    "    files_to_manipulate = find_files()  # Get files matching the criteria.\n",
    "    if len(files_to_manipulate) != 3:  # Ensure there are exactly three files to manipulate.\n",
    "        messagebox.showerror(\"Error\", \"Could not find exactly three 'clean' files for manipulation.\")\n",
    "    else:\n",
    "        try: \n",
    "            for file_path in files_to_manipulate:\n",
    "                if selected_format == 'JSON':\n",
    "                    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                        data = json.load(json_file)  # Load JSON file data.\n",
    "                        dataframe = pd.DataFrame(data[\"Records\"] if \"Records\" in data else data)  # Convert to DataFrame.\n",
    "                        if 'USER_FULL_NAME__ANONYMIZED' in dataframe.columns:\n",
    "                            # Rename the column 'USER_FULL_NAME__ANONYMIZED' to 'USER_ID'.\n",
    "                            dataframe.rename(columns={'USER_FULL_NAME__ANONYMIZED': 'USER_ID'}, inplace=True)\n",
    "                            data[\"Records\"] = dataframe.to_dict(orient=\"records\")\n",
    "                    with open(file_path, \"w\") as file:\n",
    "                        json.dump(data, file, indent=4)  # Save updated JSON data.\n",
    "                elif selected_format == 'XML':\n",
    "                    dataframe = pd.read_xml(file_path)  # Read XML file into DataFrame.\n",
    "                    if 'USER_FULL_NAME__ANONYMIZED' in dataframe.columns:\n",
    "                        # Rename the column 'USER_FULL_NAME__ANONYMIZED' to 'USER_ID'.\n",
    "                        dataframe.rename(columns={'USER_FULL_NAME__ANONYMIZED': 'USER_ID'}, inplace=True)\n",
    "                        dataframe.to_xml(file_path, index=False, root_name='Records', row_name='Record')  # Save XML.\n",
    "\n",
    "            messagebox.showinfo(\"Success\", \"Records renamed successfully!\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to rename records: {e}\")\n",
    "\n",
    "def merge():\n",
    "    selected_format = format_var.get()  # Get selected file format.\n",
    "    files_to_manipulate = find_files()  # Fetch files for processing.\n",
    "    merged_file_name = f\"merged_dataset.{selected_format.lower()}\"  # Create merged file name.\n",
    "    merged_file_path = os.path.join(dataset_location, merged_file_name)  # Generate full file path.\n",
    "\n",
    "    # Check if merged file already exists.\n",
    "    if os.path.exists(merged_file_path):\n",
    "        messagebox.showinfo(\"Info\", f\"The merged file '{merged_file_name}' already exists.\")\n",
    "    else:\n",
    "        try:\n",
    "            activity_df = None  # Placeholder for activity data.\n",
    "            user_df = None  # Placeholder for user data.\n",
    "\n",
    "            # Process files based on selected format.\n",
    "            for file_path in files_to_manipulate:\n",
    "                if selected_format == 'JSON':\n",
    "                    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                        data = json.load(json_file)\n",
    "                        df = pd.DataFrame(data[\"Records\"] if \"Records\" in data else data)\n",
    "                        # Assign to activity or user DataFrame based on columns.\n",
    "                        if 'COMPONENT' in df.columns and 'USER_ID' in df.columns:\n",
    "                            activity_df = df\n",
    "                        elif 'COMPONENT' not in df.columns and 'USER_ID' in df.columns:\n",
    "                            user_df = df\n",
    "                elif selected_format == 'XML':\n",
    "                    df = pd.read_xml(file_path)\n",
    "                    # Assign to activity or user DataFrame based on columns.\n",
    "                    if 'COMPONENT' in df.columns and 'USER_ID' in df.columns:\n",
    "                        activity_df = df\n",
    "                    elif 'COMPONENT' not in df.columns and 'USER_ID' in df.columns:\n",
    "                        user_df = df\n",
    "\n",
    "            # Ensure both DataFrames are available.\n",
    "            if activity_df is None or user_df is None:\n",
    "                raise ValueError(\"Both file columns have not been renamed yet.\")\n",
    "\n",
    "            # Merge activity and user DataFrames.\n",
    "            merged_df = pd.merge(user_df, activity_df, left_index=True, right_index=True, how='right')\n",
    "            # Resolve conflicting USER_ID columns.\n",
    "            merged_df['USER_ID'] = merged_df.apply(lambda x: x['USER_ID_y'] if x['USER_ID_y'] != 'Unknown' \n",
    "                                                   else x['USER_ID_x'], axis=1)\n",
    "            merged_df.drop(columns=['USER_ID_x', 'USER_ID_y'], inplace=True)\n",
    "\n",
    "            # Save merged data based on selected format.\n",
    "            if selected_format == 'JSON':\n",
    "                merged_data = {\"Records\": merged_df.to_dict(orient=\"records\")}\n",
    "                with open(merged_file_path, \"w\") as file:\n",
    "                    json.dump(merged_data, file, indent=4)\n",
    "            elif selected_format == 'XML':\n",
    "                merged_df.to_xml(merged_file_path, index=False, root_name='Records', row_name='Record')\n",
    "\n",
    "            update_dropdown(merged_file_name)  # Update UI with new file.\n",
    "            messagebox.showinfo(\"Success\", \"Logs merged successfully!\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to merge records: {e}\")\n",
    "\n",
    "def reshape():\n",
    "    selected_format = format_var.get()  # Get the selected file format.\n",
    "    merged_file_name = f\"merged_dataset.{selected_format.lower()}\"  # Define merged file name.\n",
    "    merged_file_path = os.path.join(dataset_location, merged_file_name)  # Get path for merged file.\n",
    "    reshaped_file_name = f\"merged_dataset_wide.{selected_format.lower()}\"  # Define reshaped file name.\n",
    "    reshaped_file_path = os.path.join(dataset_location, reshaped_file_name)  # Get path for reshaped file.\n",
    "\n",
    "    # Check if the merged file exists.\n",
    "    if os.path.exists(merged_file_path):\n",
    "        try:\n",
    "            if selected_format == 'JSON':\n",
    "                pivoted_df = None  # Placeholder for reshaped DataFrame.\n",
    "\n",
    "                # Load merged JSON file into DataFrame.\n",
    "                with open(merged_file_path, 'r', encoding='utf-8') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    df = pd.DataFrame(data[\"Records\"] if \"Records\" in data else data)\n",
    "\n",
    "                    # Add a 'MONTH' column by parsing the 'DATE' column.\n",
    "                    df['MONTH'] = pd.to_datetime(df['DATE'], format='%d/%m/%Y').dt.to_period('M')\n",
    "\n",
    "                    # Reshape data using a pivot table.\n",
    "                    pivoted_df = df.pivot_table(\n",
    "                        index=['USER_ID', 'MONTH', 'COMPONENT'],\n",
    "                        values=['TIME', 'ACTION', 'TARGET'],\n",
    "                        aggfunc=lambda x: list(x)\n",
    "                    ).reset_index()\n",
    "\n",
    "                # Create JSON structure for reshaped data.\n",
    "                output = {\"Records\": []}\n",
    "                for index, group in pivoted_df.groupby(['USER_ID', 'MONTH']):\n",
    "                    user_id = group['USER_ID'].iloc[0]\n",
    "                    date_month = group['MONTH'].iloc[0].strftime('%Y-%m')\n",
    "                    components = []\n",
    "\n",
    "                    # Group and format component interactions.\n",
    "                    for index, row in group.iterrows():\n",
    "                        component_name = row['COMPONENT']\n",
    "                        interactions = [\n",
    "                            {f\"USE{i+1}\": {\"TIME\": time, \"ACTION\": action, \"TARGET\": target}}\n",
    "                            for i, (time, action, target) in enumerate(zip(row['TIME'], row['ACTION'], row['TARGET']))\n",
    "                        ]\n",
    "                        components.append({component_name: interactions})\n",
    "\n",
    "                    # Append user records to the JSON output.\n",
    "                    output[\"Records\"].append({\"USER_ID\": user_id,\"DATE\": date_month,\"COMPONENTS\": components})\n",
    "\n",
    "                # Write reshaped data to JSON file.\n",
    "                with open(reshaped_file_path, \"w\") as file:\n",
    "                    json.dump(output, file, indent=4)\n",
    "\n",
    "            elif selected_format == 'XML':\n",
    "                # Load merged XML file into DataFrame.\n",
    "                df = pd.read_xml(merged_file_path)\n",
    "\n",
    "                # Add a 'MONTH' column by parsing the 'DATE' column.\n",
    "                df['MONTH'] = pd.to_datetime(df['DATE'], format='%d/%m/%Y').dt.to_period('M')\n",
    "\n",
    "                # Reshape data using a pivot table.\n",
    "                pivoted_df = df.pivot_table(\n",
    "                    index=['USER_ID', 'MONTH', 'COMPONENT'],\n",
    "                    values=['TIME', 'ACTION', 'TARGET'],\n",
    "                    aggfunc=lambda x: list(x)\n",
    "                ).reset_index()\n",
    "\n",
    "                # Create XML structure for reshaped data.\n",
    "                root = ET.Element('Records')\n",
    "                for index, group in pivoted_df.groupby(['USER_ID', 'MONTH']):\n",
    "                    user_id = group['USER_ID'].iloc[0]\n",
    "                    date_month = group['MONTH'].iloc[0].strftime('%Y-%m')\n",
    "                    \n",
    "                    # Create XML record for each user and month.\n",
    "                    record = ET.SubElement(root, \"Record\")\n",
    "                    user_id_elem = ET.SubElement(record, \"USER_ID\")\n",
    "                    user_id_elem.text = str(user_id)\n",
    "                    date_elem = ET.SubElement(record, \"DATE\")\n",
    "                    date_elem.text = date_month\n",
    "                    components_elem = ET.SubElement(record, \"COMPONENTS\")\n",
    "\n",
    "                    # Add component interactions to the XML.\n",
    "                    for index, row in group.iterrows():\n",
    "                        component_name = row['COMPONENT']\n",
    "                        component_elem = ET.SubElement(components_elem, component_name)\n",
    "                        for i, (time, action, target) in enumerate(zip(row['TIME'], row['ACTION'], row['TARGET'])):\n",
    "                            use_elem = ET.SubElement(component_elem, f\"USE{i+1}\")\n",
    "                            time_elem = ET.SubElement(use_elem, \"TIME\")\n",
    "                            time_elem.text = time\n",
    "                            action_elem = ET.SubElement(use_elem, \"ACTION\")\n",
    "                            action_elem.text = action\n",
    "                            target_elem = ET.SubElement(use_elem, \"TARGET\")\n",
    "                            target_elem.text = target\n",
    "\n",
    "                # Format and save reshaped data to XML file.\n",
    "                format_xml(root)\n",
    "                tree = ET.ElementTree(root)\n",
    "                tree.write(reshaped_file_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "            update_dropdown(reshaped_file_name)  # Update UI with the reshaped file.\n",
    "            messagebox.showinfo(\"Success\", \"Merged data reshaped successfully!\")\n",
    "        except Exception as e:\n",
    "            # Handle errors during reshaping.\n",
    "            messagebox.showerror(\"Error\", f\"Failed to reshape the data: {e}\")\n",
    "    else:\n",
    "        # Notify user if the merged file doesn't exist.\n",
    "        messagebox.showinfo(\"Info\", f\"The merged file '{merged_file_name}' does not exist.\")\n",
    "        \n",
    "def count():\n",
    "    selected_format = format_var.get()  # Get the selected file format.\n",
    "    wide_file_name = f\"merged_dataset_wide.{selected_format.lower()}\"  # Define wide file name.\n",
    "    wide_file_path = os.path.join(dataset_location, wide_file_name)  # Get path for wide file.\n",
    "\n",
    "    # Check if the wide file exists.\n",
    "    if os.path.exists(wide_file_path):\n",
    "        try:\n",
    "            if selected_format == 'JSON':\n",
    "                # Load the wide JSON file into a Python dictionary.\n",
    "                with open(wide_file_path, 'r', encoding='utf-8') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                output = data[\"Records\"]\n",
    "                # Check if counts already exist in the data.\n",
    "                for record in output:\n",
    "                    components = record['COMPONENTS']\n",
    "                    for component in components:\n",
    "                        for name, interactions in list(component.items()):\n",
    "                            if name == 'COUNT':\n",
    "                                messagebox.showinfo(\"Info\", \"Counts already exist!\")  # Inform user counts are present.\n",
    "                                return  \n",
    "                # Add count of interactions for each component.\n",
    "                for record in output:\n",
    "                    components = record['COMPONENTS']\n",
    "                    for component in components:\n",
    "                        for name, interactions in list(component.items()):\n",
    "                            component['COUNT'] = len(interactions)  # Set count for component.\n",
    "                # Write the updated data back to the JSON file.\n",
    "                with open(wide_file_path, \"w\") as file:\n",
    "                    json.dump(output, file, indent=4)\n",
    "\n",
    "            elif selected_format == 'XML':\n",
    "                # Parse the wide XML file and get the root element.\n",
    "                tree = ET.parse(wide_file_path)\n",
    "                root = tree.getroot()\n",
    "                # Check if counts already exist in XML elements.\n",
    "                for record in root.findall('Record'):\n",
    "                    components_elem = record.find('COMPONENTS')\n",
    "                    if components_elem is not None:\n",
    "                        for component_elem in components_elem:\n",
    "                            if component_elem.find('COUNT') is not None:\n",
    "                                messagebox.showinfo(\"Info\", \"Counts already exist!\")  # Inform user counts are present.\n",
    "                                return\n",
    "                # Add count of child elements (interactions) for each component in XML.\n",
    "                for record in root.findall('Record'):\n",
    "                    components_elem = record.find('COMPONENTS')\n",
    "                    if components_elem is not None:\n",
    "                        for component_elem in components_elem:\n",
    "                            count = len(component_elem.findall('*'))  # Get interaction count.\n",
    "                            count_elem = ET.SubElement(component_elem, 'COUNT')  # Add count as new element.\n",
    "                            count_elem.text = str(count)\n",
    "\n",
    "                # Format and save updated XML data.\n",
    "                format_xml(root)\n",
    "                with open(wide_file_path, 'w', encoding='utf-8') as xml_file:\n",
    "                    xml_file.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "                    xml_file.write(ET.tostring(root, encoding=\"unicode\", method=\"xml\"))\n",
    "\n",
    "            # Update UI and inform user of success.\n",
    "            info_label = ttk.Label(text_frame2, text=\"Proceed to Statistics Tab to produce Statistics and Correlation\",\n",
    "                                  anchor='center', relief=\"solid\", borderwidth=1, background='lightgreen')\n",
    "            info_label.grid(row=21, column=0, padx=5, pady=5, sticky='NEWS')\n",
    "\n",
    "            update_stat_dropdown()  # Update dropdown with new stats.\n",
    "            messagebox.showinfo(\"Success\", \"Monthly counts have been added successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle errors during count addition.\n",
    "            messagebox.showerror(\"Error\", f\"Failed to add counts: {e}\")\n",
    "    else:\n",
    "        # Notify user if the wide file doesn't exist.\n",
    "        messagebox.showinfo(\"Info\", f\"The wide file '{wide_file_name}' does not exist.\")\n",
    "\n",
    "# Options frame to hold the buttons\n",
    "options_frame = ttk.Frame(manipulation_tab, padding=10, relief=\"ridge\")\n",
    "options_frame.grid(row=0, column=0, padx=10, pady=10, sticky='NEWS')\n",
    "\n",
    "# Text frame to hold the contents\n",
    "text_frame2 = ttk.Frame(manipulation_tab, padding=10, relief=\"ridge\")\n",
    "text_frame2.grid(row=0, column=1, padx=10, pady=10, sticky='NEWS')\n",
    "\n",
    "# Configuring frame layouts\n",
    "for i in range(0, 22):\n",
    "    options_frame.rowconfigure(i, weight=1)\n",
    "    text_frame2.rowconfigure(i, weight=1)\n",
    "for i in range(0, 2):\n",
    "    options_frame.columnconfigure(i, weight=1)\n",
    "text_frame2.columnconfigure(0, weight=1)\n",
    "\n",
    "# Text label for file contents\n",
    "manipulation_label = ttk.Label(text_frame2, text=\"File Contents\", anchor='center', relief=\"solid\", borderwidth=1)\n",
    "manipulation_label.grid(row=0, column=0, padx=5, pady=5, sticky='NEWS')\n",
    "\n",
    "# Text widget to display manipulation contents\n",
    "manipulation_content = tk.Text(text_frame2, wrap=tk.WORD, state=tk.DISABLED, relief=\"solid\", borderwidth=1)\n",
    "manipulation_content.grid(row=1, column=0, rowspan=20, padx=5, pady=5, sticky=\"NEWS\")\n",
    "\n",
    "# Format label\n",
    "format_label = ttk.Label(options_frame, text=\"Choose Output Format:\", anchor='center', style='Custom.TLabel')\n",
    "format_label.grid(row=7, column=0, columnspan=2, padx=5, pady=5, sticky='EW')\n",
    "\n",
    "# Radiobuttons for selecting format\n",
    "format_var = tk.StringVar(value=\"JSON\")\n",
    "json_button2 = tk.Radiobutton(options_frame, text=\"JSON\", variable=format_var, value=\"JSON\", command=format_selected)\n",
    "xml_button2 = tk.Radiobutton(options_frame, text=\"XML\", variable=format_var, value=\"XML\", command=format_selected)\n",
    "json_button2.grid(row=8, column=0, sticky=\"EW\", padx=5, pady=2)\n",
    "xml_button2.grid(row=8, column=1, sticky=\"EW\", padx=5, pady=2)\n",
    "\n",
    "# Dropdown menu for file viewing\n",
    "file_var2 = tk.StringVar()\n",
    "file_var2.set(\"\")\n",
    "files2 = find_files2()\n",
    "if len(files2) == 0:\n",
    "    view_files = tk.OptionMenu(options_frame, file_var2, \"\")\n",
    "else:\n",
    "    view_files = tk.OptionMenu(options_frame, file_var2, *files2, \n",
    "                               command=lambda event_b: selected_file(event=event_b, target_widget=manipulation_content))\n",
    "view_files.configure(bg='lightblue', relief='raised')\n",
    "view_files.grid(row=9, column=0, columnspan=2, sticky='EW')\n",
    "\n",
    "# Selected file label\n",
    "selection_label2 = ttk.Label(options_frame, text='SELECTED FILE: ', anchor='center', background='white')\n",
    "selection_label2.grid(row=10, column=0, columnspan=2, padx=5, pady=5, sticky='EW')\n",
    "\n",
    "# Remove button\n",
    "remove_button = tk.Button(options_frame, text=\"REMOVE\", command=remove_records, bg=\"lightblue\", relief=\"raised\")\n",
    "remove_button.grid(row=11, column=0, columnspan=2, sticky=\"EW\", padx=5, pady=10)\n",
    "\n",
    "#Rename button\n",
    "rename_button = tk.Button(options_frame, text=\"RENAME\", command=rename, bg=\"lightblue\", relief=\"raised\")\n",
    "rename_button.grid(row=12, column=0, columnspan=2, sticky=\"EW\", padx=5, pady=10)\n",
    "\n",
    "#Merge button\n",
    "merge_button = tk.Button(options_frame, text=\"MERGE\", command=merge, bg=\"lightblue\", relief=\"raised\")\n",
    "merge_button.grid(row=13, column=0, columnspan=2, sticky=\"EW\", padx=5, pady=10)\n",
    "\n",
    "# Reshape button\n",
    "reshape_button = tk.Button(options_frame, text=\"RESHAPE\", command=reshape, bg=\"lightblue\", relief=\"raised\")\n",
    "reshape_button.grid(row=14, column=0, columnspan=2, sticky=\"EW\", padx=5, pady=10)\n",
    "\n",
    "#Count button\n",
    "count_button = tk.Button(options_frame, text=\"COUNT\", command=count, bg=\"lightblue\", relief=\"raised\")\n",
    "count_button.grid(row=15, column=0, columnspan=2, sticky=\"EW\", padx=5, pady=10)\n",
    "\n",
    "# Configure grid weights for the main window\n",
    "manipulation_tab.rowconfigure(0, weight=1)\n",
    "manipulation_tab.columnconfigure(0, weight=1)\n",
    "manipulation_tab.columnconfigure(1, weight=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "febd8077-3544-45c8-b791-2e2e0cc3b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stat_dropdown():\n",
    "    global wide_dropdown\n",
    "    # Get the list of wide files in the dataset location.\n",
    "    wide_files = [file for file in os.listdir(dataset_location) \n",
    "                  if os.path.isfile(os.path.join(dataset_location, file)) \n",
    "                  and file.startswith('merged_dataset_wide.')]\n",
    "\n",
    "    # Destroy the previous dropdown and reset its value.\n",
    "    wide_dropdown.destroy()\n",
    "    wide_file.set(\"\")\n",
    "    \n",
    "    # Create a new dropdown with the wide files if any exist.\n",
    "    if wide_files:\n",
    "        wide_dropdown = tk.OptionMenu(dropdown_frame2, wide_file, *wide_files)\n",
    "    else:\n",
    "        wide_dropdown = tk.OptionMenu(dropdown_frame2, wide_file, \"\")\n",
    "    # Customize the dropdown appearance and add it to the grid.\n",
    "    wide_dropdown.configure(bg='lightblue', relief='raised')\n",
    "    wide_dropdown.grid(row=10, column=0, sticky='EW')\n",
    "\n",
    "def extract_statistics(event=None):\n",
    "    # Get the file name from the event or dropdown.\n",
    "    file_selected = event if event else wide_file.get()\n",
    "    file_path = os.path.join(dataset_location, file_selected)\n",
    "    \n",
    "    # Check if the file exists.\n",
    "    if os.path.exists(file_path):\n",
    "        records = []\n",
    "        try:\n",
    "            extension = os.path.splitext(file_selected)[1].lower()\n",
    "            # Handle JSON files: load data and process it.\n",
    "            if extension == '.json':\n",
    "                with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                for record in data:\n",
    "                    user_id = record['USER_ID']\n",
    "                    date = record['DATE']\n",
    "                    for component in record['COMPONENTS']:\n",
    "                        if component == component['COUNT']:\n",
    "                            continue\n",
    "                        for name, interactions in component.items():\n",
    "                            count = component['COUNT']\n",
    "                            if isinstance(interactions, list):\n",
    "                                for interaction in interactions:\n",
    "                                    if not any(d['USER_ID'] == user_id and d['DATE'] == date \n",
    "                                               and d['COMPONENT'] == name for d in records):\n",
    "                                        records.append({ 'USER_ID': user_id, 'DATE': date,\n",
    "                                                        'COMPONENT': name, 'COUNT': count })\n",
    "                            else:\n",
    "                                if not any(d['USER_ID'] == user_id and d['DATE'] == date \n",
    "                                           and d['COMPONENT'] == name for d in records):\n",
    "                                    records.append({ 'USER_ID': user_id, 'DATE': date, \n",
    "                                                    'COMPONENT': name, 'COUNT': count })\n",
    "            # Handle XML files: parse and process data.\n",
    "            elif extension == '.xml':\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                for record in root.findall('Record'):\n",
    "                    user_id = record.find('USER_ID').text\n",
    "                    date = record.find('DATE').text\n",
    "                    components= record.find('COMPONENTS')\n",
    "                    if components is not None:\n",
    "                        for component in components:\n",
    "                            name = component.tag\n",
    "                            if name == 'COUNT':\n",
    "                                continue\n",
    "                            count = int(component.find('COUNT').text) if component.find('COUNT') is not None else 0\n",
    "                            for interaction in component:\n",
    "                                if interaction.tag != 'COUNT':\n",
    "                                    if not any(d['USER_ID'] == user_id and d['DATE'] == date \n",
    "                                               and d['COMPONENT'] == name for d in records):\n",
    "                                        records.append({'USER_ID': user_id, 'DATE': date, \n",
    "                                                        'COMPONENT': name, 'COUNT': count})\n",
    "            # Return the processed data as a DataFrame if records are found.\n",
    "            if records:\n",
    "                df = pd.DataFrame(records)\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to extract statistics: {e}\")\n",
    "        return df\n",
    "    else:\n",
    "        # Notify user if the file does not exist.\n",
    "        messagebox.showinfo(\"Info\", f\"File '{file_selected}' does not exist.\")\n",
    "\n",
    "def create_table(parent, data, title):\n",
    "    # Create and display a table with the given data in the parent container.\n",
    "    title_label = tk.Label(parent, text=title, font=('Arial', 10, 'bold'), bg=\"lightgrey\")\n",
    "    title_label.pack(fill=tk.BOTH, padx=5, pady=5)\n",
    "    tree = ttk.Treeview(parent, columns=list(data.columns), show='headings', height=5)\n",
    "    \n",
    "    # Set up column headings.\n",
    "    for col in data.columns:\n",
    "        tree.heading(col, text=col)\n",
    "        tree.column(col, width=30, anchor=tk.CENTER)\n",
    "    \n",
    "    # Insert the data into the table.\n",
    "    for index, row in data.iterrows():\n",
    "        tree.insert(\"\", \"end\", values=list(row))\n",
    "    \n",
    "    tree.pack(fill=tk.BOTH, padx=5, pady=5)\n",
    "    return tree\n",
    "\n",
    "def create_bar_chart(df, title, parent):\n",
    "    # Group data by date and component, and sum the counts.\n",
    "    df_grouped = df.groupby(['DATE', 'COMPONENT'])['COUNT'].sum().unstack(fill_value=0)\n",
    "    \n",
    "    # Create a stacked bar chart.\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    df_grouped.plot(kind='bar', stacked=True, ax=ax, colormap='Set3')\n",
    "\n",
    "    # Set chart title and axis labels.\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Number of Interactions\")\n",
    "    ax.legend(title=\"Component\")\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "    \n",
    "    # Display the chart in the parent container.\n",
    "    canvas = FigureCanvasTkAgg(fig, master=parent)\n",
    "    canvas.draw()\n",
    "    canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "def output_statistics():\n",
    "    try:\n",
    "        # Extract statistics and generate the output if successful.\n",
    "        df = extract_statistics()\n",
    "        if df is not None or not df.empty:\n",
    "            components = ['QUIZ', 'LECTURE', 'ASSIGNMENT', 'ATTENDENCE', 'SURVEY']\n",
    "            df_components = df[df['COMPONENT'].isin(components)]\n",
    "            \n",
    "            # Calculate statistics per month.\n",
    "            stats_per_month = (\n",
    "                df_components.groupby(['DATE', 'COMPONENT'])['COUNT']\n",
    "                .agg(MEAN='mean', MEDIAN='median', MODE=lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "                .reset_index())\n",
    "            stats_per_month['MEAN'] = stats_per_month['MEAN'].round(1)\n",
    "            # Calculate overall statistics.\n",
    "            stats_overall = (\n",
    "                df_components.groupby(['COMPONENT'])['COUNT']\n",
    "                .agg(MEAN='mean', MEDIAN='median', MODE=lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "                .reset_index())\n",
    "            stats_overall['MEAN'] = stats_overall['MEAN'].round(1)\n",
    "\n",
    "            # Clear previous widgets and display the statistics.\n",
    "            for widget in output_frame.winfo_children():\n",
    "                widget.destroy()\n",
    "            create_table(output_frame, stats_per_month, \"Statistics per Month\")\n",
    "            create_table(output_frame, stats_overall, \"Statistics per Semester\")\n",
    "            create_bar_chart(df_components, \"All Interactions per Component\", output_frame)\n",
    "            \n",
    "            messagebox.showinfo(\"Success\", \"Statistics have been generated!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Failed to extract statistics\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", \"Failed to generate statistics\")\n",
    "\n",
    "def output_correlation():\n",
    "    try:\n",
    "        # Extract statistics and generate correlation matrix if successful.\n",
    "        df = extract_statistics()\n",
    "        if df is not None or not df.empty:\n",
    "            components = ['QUIZ', 'LECTURE', 'ASSIGNMENT', 'ATTENDENCE', 'SURVEY']\n",
    "            df_components = df[df['COMPONENT'].isin(components)]\n",
    "            \n",
    "            # Create pivot table and correlation matrix.\n",
    "            pivot_table = df_components.pivot_table(index='USER_ID', columns='COMPONENT', \n",
    "                                                  values='COUNT', aggfunc='sum', fill_value=0)\n",
    "            pivot_table = pivot_table.reset_index()\n",
    "            correlation_matrix = pivot_table.corr()\n",
    "            \n",
    "            # Create and display the heatmap for correlation.\n",
    "            fig, ax = plt.subplots(figsize=(8, 7))\n",
    "            sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n",
    "            \n",
    "            # Clear previous widgets and display the correlation heatmap.\n",
    "            for widget in output_frame.winfo_children():\n",
    "                widget.destroy()\n",
    "            canvas = FigureCanvasTkAgg(fig, master=output_frame) \n",
    "            canvas.draw()\n",
    "            canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)\n",
    "            \n",
    "            # Generate a summary of significant correlations.\n",
    "            summary_text = \"Correlation Summary:\\n\"\n",
    "            significant_correlations = []\n",
    "            \n",
    "            user_id_correlations = correlation_matrix['USER_ID'].drop('USER_ID')\n",
    "            for component, correlation in user_id_correlations.items():\n",
    "                if abs(correlation) >= 0.7:\n",
    "                    significant_correlations.append(f\"Significant correlation between USER_ID and {component}: {correlation:.2f}\")\n",
    "            if significant_correlations:\n",
    "                summary_text += \"\\n\".join(significant_correlations)\n",
    "            else:\n",
    "                summary_text += \"No significant correlations found between USER_ID and COMPONENTS.\"\n",
    "            summary_label = tk.Label(output_frame, text=summary_text, anchor=\"w\", \n",
    "                                     justify=\"left\", font=(\"Arial\", 10))\n",
    "            summary_label.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "            \n",
    "            messagebox.showinfo(\"Success\", \"Correlation has been generated!\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Failed to extract statistics\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", \"Failed to generate correlation\")\n",
    "\n",
    "# Dropdown frame\n",
    "dropdown_frame2 = ttk.Frame(statistics_tab, padding=10, relief=\"ridge\")\n",
    "dropdown_frame2.grid(row=0, column=0, padx=10, pady=10, sticky='NEWS')\n",
    "\n",
    "#Text frame\n",
    "text_frame3 = ttk.Frame(statistics_tab, padding=10, relief=\"ridge\")\n",
    "text_frame3.grid(row=0, column=1, padx=10, pady=10, sticky='NEWS')\n",
    "\n",
    "# Canvas widget\n",
    "output_frame = tk.Canvas(text_frame3, bg='white', relief=\"ridge\")\n",
    "output_frame.grid(row=1, rowspan=20, column=0, padx=10, pady=10, sticky='NEWS')\n",
    "\n",
    "# Output label\n",
    "outputs_label = ttk.Label(text_frame3, text=\"File Outputs\", anchor='center', relief=\"solid\", borderwidth=1)\n",
    "outputs_label.grid(row=0, column=0, padx=5, pady=5, sticky='NEWS')\n",
    "\n",
    "# Confihure frame weights\n",
    "for i in range(0, 22):\n",
    "    dropdown_frame2.rowconfigure(i, weight=1)\n",
    "    text_frame3.rowconfigure(i, weight=1)\n",
    "dropdown_frame2.columnconfigure(0, weight=1)\n",
    "text_frame3.columnconfigure(0, weight=1)\n",
    "\n",
    "# Configure the tab weights\n",
    "statistics_tab.rowconfigure(0, weight=1)\n",
    "statistics_tab.columnconfigure(0, weigh=1)\n",
    "statistics_tab.columnconfigure(1, weigh=2)\n",
    "\n",
    "# Options label\n",
    "format_label2 = ttk.Label(dropdown_frame2, text=\"Choose A File:\", anchor='center', style='Custom.TLabel')\n",
    "format_label2.grid(row=9, column=0, columnspan=2, padx=5, pady=5, sticky='EW')\n",
    "\n",
    "# Creating a dropdown of files used for statistics\n",
    "wide_file = tk.StringVar()\n",
    "wide_file.set(\"\")\n",
    "wide_files = [file for file in os.listdir(dataset_location) if os.path.isfile(os.path.join(dataset_location, file))\n",
    "              and file.startswith('merged_dataset_wide.')]\n",
    "if not wide_files:\n",
    "    wide_dropdown = tk.OptionMenu(dropdown_frame2, wide_file, \"\")\n",
    "else:\n",
    "    wide_dropdown = tk.OptionMenu(dropdown_frame2, wide_file, *wide_files)\n",
    "wide_dropdown.configure(bg='lightblue', relief='raised')\n",
    "wide_dropdown.grid(row=10, column=0, sticky='EW')\n",
    "\n",
    "# Statistics button\n",
    "statistics_button = tk.Button(dropdown_frame2, text=\"OUTPUT STATISTICS\", command=output_statistics, \n",
    "                              bg=\"lightblue\", relief=\"raised\")\n",
    "statistics_button.grid(row=11, column=0, sticky=\"EW\", padx=5, pady=10)\n",
    "\n",
    "# Correlation button\n",
    "correlation_button = tk.Button(dropdown_frame2, text=\"OUTPUT CORRELATION\", command=output_correlation, \n",
    "                               bg=\"lightblue\", relief=\"raised\")\n",
    "correlation_button.grid(row=12, column=0, sticky=\"EW\", padx=5, pady=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b24b220-2065-456f-b1c9-f6986f4ec1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the GUI\n",
    "gui.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca1759-8bbb-4bf3-bcdb-0984066a4b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4752ce5-f1b7-40bf-9ce9-12d892f7bdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
